{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/evanm31/p4k-scraper/blob/master/data/scrapefork.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install retry\n",
    "# !python -m pip install backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and set up enviornment\n",
    "import urllib\n",
    "import re\n",
    "import time\n",
    "import retry\n",
    "import backoff\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "if os.name == \"posix\":\n",
    "    os.chdir(\"/Users/kylezengo/Desktop/DS/Music Ratings/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_info(album_link):\n",
    "    '''\n",
    "    This function parses the HTML of the page and attempts to gather attributes like artist name, album, genre,\n",
    "    date, and the review text itself, instead inputting a null value if the requested element is not found on\n",
    "    the page. All of the data are put into a Pandas dataframe and returned for use in the gather function.\n",
    "    VARIABLES\n",
    "    album_link - A string that refers to the album section of a link to a Pitchfork review.\n",
    "    e.g. '/reviews/albums/neil-young-promise-of-the-real-visitor/'\n",
    "    '''\n",
    "    page = requests.get(album_link) #request URL\n",
    "    soup = BeautifulSoup(page.content, 'html.parser') #parse with beautifulsoup\n",
    "\n",
    "    status = True\n",
    "    while status:\n",
    "        if page.status_code != 200:\n",
    "            print(\"Error: \",page.status_code)\n",
    "            time.sleep(2)\n",
    "            page = requests.get(album_link) #request URL\n",
    "            soup = BeautifulSoup(page.content, 'html.parser') #parse with beautifulsoup\n",
    "        else:\n",
    "            status = False\n",
    "\n",
    "    title = str(soup.find('title').string) #album and artist \n",
    "    sents = [element.text for element in soup.find_all('p')] #cleaned text output\n",
    "    all_text = \" \".join(sents)\n",
    "    selected_text = all_text.split('Reviewed: ',1)[1]\n",
    "    selected_text = selected_text.split(\" \",3)\n",
    "    review_text = selected_text[3]\n",
    "    review_text = review_text.split(\" By signing up you agree to our User Agreement (including the class action waiver and arbitration\",1)[0]\n",
    "    \n",
    "    try:\n",
    "        score = float((soup.find(class_=\"score\").string)) #score\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            score = float((soup.find(class_=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ Rating-bkjebD iUEiRd bwCcXY imqiqZ\").string))\n",
    "        except:\n",
    "            # score = float((soup.find(class_=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ Rating-iATjmx iUEiRd bwCcXY dBMsvl\").string))\n",
    "            score = float((soup.find(class_=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ Rating-bkjebD iUEiRd bwCcXY fuVxVq\").string))\n",
    "    try:\n",
    "        genre = soup.find(class_=\"genre-list__link\").string #genre\n",
    "    except AttributeError:\n",
    "        genre = all_text.split(\"Genre: \",1)[1]\n",
    "        genre = genre.split(\"Label: \",1)[0].strip()\n",
    "    try:\n",
    "        reviewed_date = str(soup.find(class_=\"pub-date\").string) #date\n",
    "    except AttributeError:\n",
    "        reviewed_date = selected_text[0]+\" \"+selected_text[1]+\" \"+selected_text[2]\n",
    "    try:\n",
    "        artist = soup.find(class_=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ SplitScreenContentHeaderArtist-ftloCc iUEiRd Byyns kRtQWW\").string\n",
    "    except:\n",
    "        artist = get_artist(title)\n",
    "    try:\n",
    "        album = soup.find(class_=\"BaseWrap-sc-gjQpdd BaseText-ewhhUZ SplitScreenContentHeaderHed-lcUSuI iUEiRd ckzqqn fTtZlw\").string\n",
    "    except:\n",
    "        album = get_album(title)\n",
    "        \n",
    "    df = pd.DataFrame({'artist': [artist]\n",
    "                       ,'album': [album]\n",
    "                       ,'score': [score]\n",
    "                       ,'genre': [genre]\n",
    "                       ,'review': [review_text]\n",
    "                       ,'best': [1 if \"best new\" in all_text.lower() else 0]\n",
    "                       ,'reviewed_date': [reviewed_date]\n",
    "                       ,'link':[album_link]})\n",
    "    return df\n",
    "\n",
    "def get_artist(title):\n",
    "    '''\n",
    "    This function retreives the artist name from the scraped title string.\n",
    "    VARIABLES\n",
    "    title - A string of a cleaned Pitchfork album review title.\n",
    "    '''\n",
    "    artist = ''\n",
    "    for character in title:\n",
    "        #add to string up until ':' \n",
    "        if character != \":\":\n",
    "            artist += character\n",
    "        else:\n",
    "            break\n",
    "    return artist\n",
    "        \n",
    "def get_album(title):\n",
    "    '''\n",
    "    This function retreives the album name from the scraped title string.\n",
    "    VARIABLES\n",
    "    title - A string of a cleaned Pitchfork album review title.\n",
    "    ''' \n",
    "    my_str = ''\n",
    "    #find ':' and index and start there\n",
    "    index = title.find(\":\")\n",
    "    title = title[index+2:]\n",
    "    #for each character afterwards, add it until '|'\n",
    "    for character in title:\n",
    "        if character == \"|\":\n",
    "            break\n",
    "        else:\n",
    "            my_str +=character\n",
    "    album = my_str[:-14] #return just the title\n",
    "    return album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry.retry(urllib.error.URLError, tries=4, delay=3, backoff=2)\n",
    "def urlopen_with_retry(url):\n",
    "    return urlopen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of album review links\n",
    "links_file_list = os.listdir(\"pitchfork_links\")\n",
    "\n",
    "links_list = []\n",
    "for i in links_file_list:\n",
    "    links = open(\"pitchfork_links/\"+i)\n",
    "    links = links.read().splitlines()\n",
    "    \n",
    "    links_list += links\n",
    "\n",
    "links_list = list(set(links_list))\n",
    "links_list = [x for x in links_list if x.startswith(\"https\")]\n",
    "print(f'{len(links_list)} album review links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get the link of each album review\n",
    "# Takes over 14 hours to run!\n",
    "\n",
    "new_links = []\n",
    "# for i in range(1,1987): # 1987 final page on 2021-08-18\n",
    "for i in range(1,5):\n",
    "    print(str(i)+\":\",requests.get(\"https://pitchfork.com/reviews/albums/?page=\" + str(i)))\n",
    "    req = Request(\"https://pitchfork.com/reviews/albums/?page=\" + str(i))\n",
    "    html_page = urlopen_with_retry(req)\n",
    "\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    \n",
    "    for link in soup.findAll('a'):\n",
    "        link_get_href = link.get('href')\n",
    "        if link_get_href == '/reviews/albums/':\n",
    "            pass\n",
    "        elif link_get_href.startswith( '/reviews/albums/?genre=' ):\n",
    "            pass\n",
    "        elif link_get_href.startswith( '/reviews/albums/' ):\n",
    "            new_links.append(\"https://pitchfork.com\"+link_get_href)\n",
    "\n",
    "new_links = list(set(new_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save list since takes time to generate\n",
    "with open(f'pitchfork_links/pitchfork_links_{datetime.datetime.now().date()}.txt', 'w') as f:\n",
    "    for item in new_links:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test links\n",
    "# new_links = [\"https://pitchfork.com/reviews/albums/james-devane-searching/\",\n",
    "#              \"https://pitchfork.com/reviews/albums/blood-incantation-absolute-elsewhere/\",\n",
    "#              \"https://pitchfork.com/reviews/albums/body-meat-starchris/\",\n",
    "#              \"https://pitchfork.com/reviews/albums/fred-again-actual-life-3-january-1-september-9-2022/\",\n",
    "#              \"https://pitchfork.com/reviews/albums/100-gecs-10000-gecs/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dat = []\n",
    "for idx, i in enumerate(new_links):\n",
    "    print(f'{idx}: {i}')\n",
    "    dat.append(gather_info(i))\n",
    "\n",
    "reviews = pd.concat(dat).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews.to_csv(\"/Users/kylezengo/Desktop/reviews.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
